model_name: "gpt-3.5-turbo"
learning_rate: 1e-4
batch_size: 32
num_epochs: 10
max_seq_length: 2048
warmup_steps: 1000
weight_decay: 0.01
gradient_accumulation_steps: 4
fp16: True
num_train_samples: 1000000